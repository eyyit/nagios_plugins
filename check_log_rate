#!/usr/bin/env python3

import argparse
import os
import sys
import json
import time


EXIT_STATUSES = {
    0: 'OK',
    1: 'WARNING',
    2: 'CRITICAL',
    3: 'UNKNOWN',
}
MIN_REQUIRED_DELTA = 60 * 60  # 1 Hour.


def do_exit(code, msg):
  print(f'{EXIT_STATUSES[code]} - {msg}')
  sys.exit(code)


def format_size(size):
  units = ['B', 'KB', 'MB', 'GB']
  unit = units[0]
  for u in units[1:]:
    if size < 1024:
      break
    size /= 1024
    unit = u
  return f'{size:.2f} {unit}'


def get_state_path(logfile):
  safe_name = logfile.replace('/', '_').lstrip('_')
  return f'/var/tmp/nagios_log_rate_state_{safe_name}.json'


def get_state(logfile, min_age):
  statefile = get_state_path(logfile)
  timestamp = int(time.time())
  valid_samples = []
  samples = read_state(logfile, statefile)
  newest = stat_file(logfile)
  newest.update({'timestamp': timestamp})
  samples.append(newest)

  # Validate samples.
  for sample in samples:
    if all(x in sample for x in ['inode', 'size', 'timestamp']):
      # Remove samples older than age window plus some wiggle room.
      if timestamp - sample['timestamp'] <= min_age * 2:
        valid_samples.append(sample)
  valid_samples.sort(key=lambda sample: sample['timestamp'])

  with open(statefile, 'w') as f:
    json.dump(valid_samples, f)
  return valid_samples


def read_state(logfile, statefile):
  try:
    with open(statefile, 'r') as f:
      state = json.load(f)
  except Exception:
    return []
  if not isinstance(state, list):
    return []
  return state


def stat_file(filename):
  try:
    stat = os.stat(filename)
  except Exception as error:
    do_exit(3, f'Failed to stat file: {error}')
  return {'inode': stat.st_ino, 'size': stat.st_size}


parser = argparse.ArgumentParser(
  description='Check log growth rate per hour and alert if above threshold')
parser.add_argument('logfile', help='Path to the logfile to monitor')
parser.add_argument('-m', '--min-age', type=int, default=MIN_REQUIRED_DELTA,
                    help=('Minimum age of state data in seconds before '
                          f'evaluation (default: {MIN_REQUIRED_DELTA})'))
parser.add_argument('-w', '--warning', type=int, required=True,
                    help='Warning threshold in KB/min')
parser.add_argument('-c', '--critical', type=int, required=True,
                    help='Critical threshold in KB/min')

args = parser.parse_args()
logfile = args.logfile
min_age = args.min_age
warn_thresh = args.warning
crit_thresh = args.critical

if not os.path.isfile(logfile):
  do_exit(3, f'Log file not found: {logfile}')

samples = get_state(logfile, min_age)

if len(samples) < 2:
  do_exit(0, 'Waiting for more data (initializing sample history)')

oldest = samples[0]
newest = samples[-1]

delta_size = newest['size'] - oldest['size']
delta_time = newest['timestamp'] - oldest['timestamp']

if newest['inode'] != oldest['inode']:
  do_exit(0, 'Detected log rotation. Reset state.')
if delta_size < 0:
  do_exit(0, 'Detected log truncation. Reset state.')
if delta_time <= 0:
  do_exit(3, f'Invalid time delta: {delta_time} seconds')
if delta_time < min_age:
  minutes = delta_time // 60
  required = min_age // 60
  do_exit(0, (f'Waiting for more data ({minutes} minutes collected, need '
              f'{required}) before evaluating log growth rate.'))

bytes_per_sec = delta_size / delta_time
bytes_per_min = delta_size * 60 / delta_time
rate_kb_per_min = int(bytes_per_min / 1024)

if rate_kb_per_min >= crit_thresh:
  exit_code = 2
elif rate_kb_per_min >= warn_thresh:
  exit_code = 1
else:
  exit_code = 0

rate_sec_fmt = format_size(bytes_per_sec) + '/s'
rate_min_fmt = format_size(bytes_per_min) + '/min'
since_str = f'{delta_time} seconds'

do_exit(exit_code, (
  f'{logfile} grew {format_size(delta_size)} in {since_str}'
  f' -- Rate: {rate_sec_fmt} ({rate_min_fmt})'
  f' -- Thresholds (KB/min): warn: {warn_thresh}, crit: {crit_thresh}'
))
